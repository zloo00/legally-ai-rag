#!/usr/bin/env python3
"""
Benchmark –¥–ª—è RAG —Å–∏—Å—Ç–µ–º—ã —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
–ò–∑–º–µ—Ä—è–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –∫–∞—á–µ—Å—Ç–≤–æ –∏ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã
"""

import os
import time
import json
import statistics
from datetime import datetime
from typing import List, Dict, Any, Tuple
import pandas as pd
from dotenv import load_dotenv

from rag_system import EnhancedRAGSystem
from rag_factory import EnhancedRAGSystem

rag = EnhancedRAGSystem()

load_dotenv()

class RAGBenchmark:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è benchmark —Ç–µ—Å—Ç–æ–≤ RAG —Å–∏—Å—Ç–µ–º—ã"""
    
    def __init__(self, output_dir: str = "benchmark_results"):
        self.output_dir = output_dir
        self.results = []
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        os.makedirs(output_dir, exist_ok=True)
        
        # –¢–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è benchmark
        self.test_questions = [
            "–ß—Ç–æ —Ç–∞–∫–æ–µ –≥—Ä–∞–∂–¥–∞–Ω—Å–∫–æ–µ –ø—Ä–∞–≤–æ?",
            "–ö–∞–∫–∏–µ –ø—Ä–∞–≤–∞ –∏–º–µ–µ—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–∏–∫ –∏–º—É—â–µ—Å—Ç–≤–∞?",
            "–ö–∞–∫ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –¥–æ–≥–æ–≤–æ—Ä –∫—É–ø–ª–∏-–ø—Ä–æ–¥–∞–∂–∏?",
            "–ß—Ç–æ —Ç–∞–∫–æ–µ —Ç—Ä—É–¥–æ–≤–æ–π –¥–æ–≥–æ–≤–æ—Ä?",
            "–ö–∞–∫–∏–µ –æ—Å–Ω–æ–≤–∞–Ω–∏—è –¥–ª—è —Ä–∞—Å—Ç–æ—Ä–∂–µ–Ω–∏—è –±—Ä–∞–∫–∞?",
            "–ß—Ç–æ —Ç–∞–∫–æ–µ –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ –∑–∞–∫–æ–Ω—É?",
            "–ö–∞–∫–∏–µ –≤–∏–¥—ã –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—É—Å–º–æ—Ç—Ä–µ–Ω—ã –≤ –≥—Ä–∞–∂–¥–∞–Ω—Å–∫–æ–º –ø—Ä–∞–≤–µ?",
            "–ö–∞–∫ –∑–∞—â–∏—â–∞—é—Ç—Å—è –ø—Ä–∞–≤–∞ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª–µ–π?",
            "–ß—Ç–æ —Ç–∞–∫–æ–µ –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–∏–≤–Ω–∞—è –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å?",
            "–ö–∞–∫–∏–µ –ø—Ä–∞–≤–∞ –∏–º–µ–µ—Ç —Ä–∞–±–æ—Ç–Ω–∏–∫ –ø—Ä–∏ —É–≤–æ–ª—å–Ω–µ–Ω–∏–∏?",
            "–ß—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç —Å—Ç–∞—Ç—å—è 1 –ì—Ä–∞–∂–¥–∞–Ω—Å–∫–æ–≥–æ –∫–æ–¥–µ–∫—Å–∞ –†–ö?",
            "–ö–∞–∫–∏–µ –ø—Ä–∞–≤–∞ –∏–º–µ–µ—Ç —É—á–∞—Å—Ç–Ω–∏–∫ –ø–æ–ª–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä–∏—â–µ—Å—Ç–≤–∞?",
            "–ö–∞–∫ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –¥–µ–µ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –≥—Ä–∞–∂–¥–∞–Ω–∏–Ω–∞?",
            "–ß—Ç–æ —Ç–∞–∫–æ–µ –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ –∏ –∫–∞–∫ –æ–Ω–æ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç?",
            "–ö–∞–∫–∏–µ –≤–∏–¥—ã —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—É—Å–º–æ—Ç—Ä–µ–Ω—ã –≤ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–µ?",
            "–ö–∞–∫ –∑–∞—â–∏—â–∞—é—Ç—Å—è –∞–≤—Ç–æ—Ä—Å–∫–∏–µ –ø—Ä–∞–≤–∞?",
            "–ß—Ç–æ —Ç–∞–∫–æ–µ –ª–∏—Ü–µ–Ω–∑–∏–æ–Ω–Ω—ã–π –¥–æ–≥–æ–≤–æ—Ä?",
            "–ö–∞–∫–∏–µ –ø—Ä–∞–≤–∞ –∏–º–µ–µ—Ç –Ω–∞—Å–ª–µ–¥–Ω–∏–∫?",
            "–ö–∞–∫ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —Ä–∞–∑–º–µ—Ä –∞–ª–∏–º–µ–Ω—Ç–æ–≤?",
            "–ß—Ç–æ —Ç–∞–∫–æ–µ –ø—Ä–µ–∑—É–º–ø—Ü–∏—è –Ω–µ–≤–∏–Ω–æ–≤–Ω–æ—Å—Ç–∏?"
        ]
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
        self.quality_questions = [
            {
                "question": "–ß—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç —Å—Ç–∞—Ç—å—è 1 –ì—Ä–∞–∂–¥–∞–Ω—Å–∫–æ–≥–æ –∫–æ–¥–µ–∫—Å–∞ –†–ö?",
                "expected_keywords": ["–≥—Ä–∞–∂–¥–∞–Ω—Å–∫–æ–µ –ø—Ä–∞–≤–æ", "–æ—Ç–Ω–æ—à–µ–Ω–∏—è", "–∏–º—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ", "–ª–∏—á–Ω—ã–µ"],
                "expected_sources": ["–ì—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –∫–æ–¥–µ–∫—Å", "—Å—Ç–∞—Ç—å—è 1"]
            },
            {
                "question": "–ö–∞–∫–∏–µ –ø—Ä–∞–≤–∞ –∏–º–µ–µ—Ç —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–∏–∫ –∏–º—É—â–µ—Å—Ç–≤–∞?",
                "expected_keywords": ["–≤–ª–∞–¥–µ–Ω–∏–µ", "–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ", "—Ä–∞—Å–ø–æ—Ä—è–∂–µ–Ω–∏–µ", "—Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å"],
                "expected_sources": ["–ì—Ä–∞–∂–¥–∞–Ω—Å–∫–∏–π –∫–æ–¥–µ–∫—Å", "–ø—Ä–∞–≤–æ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏"]
            },
            {
                "question": "–ß—Ç–æ —Ç–∞–∫–æ–µ —Ç—Ä—É–¥–æ–≤–æ–π –¥–æ–≥–æ–≤–æ—Ä?",
                "expected_keywords": ["—Ç—Ä—É–¥–æ–≤–æ–π –¥–æ–≥–æ–≤–æ—Ä", "—Ä–∞–±–æ—Ç–Ω–∏–∫", "—Ä–∞–±–æ—Ç–æ–¥–∞—Ç–µ–ª—å", "—Ç—Ä—É–¥–æ–≤—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è"],
                "expected_sources": ["–¢—Ä—É–¥–æ–≤–æ–π –∫–æ–¥–µ–∫—Å", "—Ç—Ä—É–¥–æ–≤–æ–π –¥–æ–≥–æ–≤–æ—Ä"]
            }
        ]

    def measure_query_performance(self, rag_system, question: str, iterations: int = 3) -> Dict[str, Any]:
        """–ò–∑–º–µ—Ä—è–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
        times = []
        results = []
        
        for _ in range(iterations):
            start_time = time.time()
            try:
                result = rag_system.query(question)
                end_time = time.time()
                
                times.append(end_time - start_time)
                results.append(result)
                
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}")
                times.append(float('inf'))
                results.append(None)
        
        return {
            "question": question,
            "avg_time": statistics.mean(times),
            "min_time": min(times),
            "max_time": max(times),
            "std_time": statistics.stdev(times) if len(times) > 1 else 0,
            "success_rate": len([r for r in results if r is not None]) / len(results),
            "results": results[0] if results[0] else None
        }

    def measure_system_stats(self, rag_system) -> Dict[str, Any]:
        """–ò–∑–º–µ—Ä—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–∏—Å—Ç–µ–º—ã"""
        try:
            stats = rag_system.get_system_stats()
            return {
                "total_vectors": stats.get('total_vectors', 0),
                "index_dimension": stats.get('index_dimension', 0),
                "index_name": stats.get('index_name', 'unknown'),
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            return {"error": str(e)}

    def measure_quality_metrics(self, rag_system, question_data: Dict[str, Any]) -> Dict[str, Any]:
        """–ò–∑–º–µ—Ä—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–∞"""
        question = question_data["question"]
        expected_keywords = question_data.get("expected_keywords", [])
        expected_sources = question_data.get("expected_sources", [])
        
        try:
            result = rag_system.query(question)
            answer = result.get("answer", "")
            sources = result.get("sources", [])
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–∂–∏–¥–∞–µ–º—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            keyword_matches = sum(1 for keyword in expected_keywords 
                                if keyword.lower() in answer.lower())
            keyword_score = keyword_matches / len(expected_keywords) if expected_keywords else 0
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ–∂–∏–¥–∞–µ–º—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            source_matches = sum(1 for source in expected_sources 
                               if any(source.lower() in s.lower() for s in sources))
            source_score = source_matches / len(expected_sources) if expected_sources else 0
            
            # –î–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞
            answer_length = len(answer)
            
            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            sources_count = len(sources)
            
            return {
                "question": question,
                "keyword_score": keyword_score,
                "source_score": source_score,
                "answer_length": answer_length,
                "sources_count": sources_count,
                "answer": answer,
                "sources": sources
            }
            
        except Exception as e:
            return {
                "question": question,
                "error": str(e),
                "keyword_score": 0,
                "source_score": 0,
                "answer_length": 0,
                "sources_count": 0
            }

    def run_performance_benchmark(self, rag_system, engine_name: str = "baseline") -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç benchmark –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        print(f"üöÄ –ó–∞–ø—É—Å–∫ benchmark –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è {engine_name}")
        print("=" * 60)
        
        # –ò–∑–º–µ—Ä—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–∏—Å—Ç–µ–º—ã
        system_stats = self.measure_system_stats(rag_system)
        print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã: {system_stats}")
        
        # –ò–∑–º–µ—Ä—è–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        performance_results = []
        for i, question in enumerate(self.test_questions, 1):
            print(f"[{i}/{len(self.test_questions)}] –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: {question[:50]}...")
            result = self.measure_query_performance(rag_system, question)
            performance_results.append(result)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        avg_times = [r["avg_time"] for r in performance_results if r["avg_time"] != float('inf')]
        success_rates = [r["success_rate"] for r in performance_results]
        
        benchmark_result = {
            "engine": engine_name,
            "timestamp": datetime.now().isoformat(),
            "system_stats": system_stats,
            "total_questions": len(self.test_questions),
            "avg_query_time": statistics.mean(avg_times) if avg_times else 0,
            "min_query_time": min(avg_times) if avg_times else 0,
            "max_query_time": max(avg_times) if avg_times else 0,
            "avg_success_rate": statistics.mean(success_rates),
            "performance_results": performance_results
        }
        
        return benchmark_result

    def run_quality_benchmark(self, rag_system, engine_name: str = "baseline") -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç benchmark –∫–∞—á–µ—Å—Ç–≤–∞"""
        print(f"üéØ –ó–∞–ø—É—Å–∫ benchmark –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è {engine_name}")
        print("=" * 60)
        
        quality_results = []
        for i, question_data in enumerate(self.quality_questions, 1):
            print(f"[{i}/{len(self.quality_questions)}] –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞: {question_data['question'][:50]}...")
            result = self.measure_quality_metrics(rag_system, question_data)
            quality_results.append(result)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞—á–µ—Å—Ç–≤–∞
        keyword_scores = [r["keyword_score"] for r in quality_results if "keyword_score" in r]
        source_scores = [r["source_score"] for r in quality_results if "source_score" in r]
        answer_lengths = [r["answer_length"] for r in quality_results if "answer_length" in r]
        sources_counts = [r["sources_count"] for r in quality_results if "sources_count" in r]
        
        quality_benchmark = {
            "engine": engine_name,
            "timestamp": datetime.now().isoformat(),
            "total_questions": len(self.quality_questions),
            "avg_keyword_score": statistics.mean(keyword_scores) if keyword_scores else 0,
            "avg_source_score": statistics.mean(source_scores) if source_scores else 0,
            "avg_answer_length": statistics.mean(answer_lengths) if answer_lengths else 0,
            "avg_sources_count": statistics.mean(sources_counts) if sources_counts else 0,
            "quality_results": quality_results
        }
        
        return quality_benchmark

    def run_load_test(self, rag_system, concurrent_queries: int = 5, duration_seconds: int = 60) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –Ω–∞–≥—Ä—É–∑–æ—á–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ"""
        print(f"‚ö° –ó–∞–ø—É—Å–∫ –Ω–∞–≥—Ä—É–∑–æ—á–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è {concurrent_queries} –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
        print("=" * 60)
        
        import threading
        import queue
        
        results_queue = queue.Queue()
        start_time = time.time()
        
        def worker():
            while time.time() - start_time < duration_seconds:
                question = self.test_questions[hash(str(time.time())) % len(self.test_questions)]
                try:
                    result = rag_system.query(question)
                    results_queue.put({
                        "success": True,
                        "time": time.time() - start_time,
                        "question": question,
                        "result": result
                    })
                except Exception as e:
                    results_queue.put({
                        "success": False,
                        "time": time.time() - start_time,
                        "question": question,
                        "error": str(e)
                    })
                time.sleep(0.1)  # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º worker'—ã
        threads = []
        for _ in range(concurrent_queries):
            thread = threading.Thread(target=worker)
            thread.start()
            threads.append(thread)
        
        # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        time.sleep(duration_seconds)
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = []
        while not results_queue.empty():
            results.append(results_queue.get())
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        successful_queries = [r for r in results if r["success"]]
        failed_queries = [r for r in results if not r["success"]]
        
        load_test_result = {
            "concurrent_queries": concurrent_queries,
            "duration_seconds": duration_seconds,
            "total_queries": len(results),
            "successful_queries": len(successful_queries),
            "failed_queries": len(failed_queries),
            "success_rate": len(successful_queries) / len(results) if results else 0,
            "queries_per_second": len(results) / duration_seconds,
            "results": results
        }
        
        return load_test_result

    def run_full_benchmark(self, engine_name: str = "baseline") -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π benchmark"""
        print(f"üéØ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ benchmark –¥–ª—è {engine_name}")
        print("=" * 80)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º RAG —Å–∏—Å—Ç–µ–º—É
        try:
            if engine_name == "baseline":
                rag_system = EnhancedRAGSystem()
            else:
                rag_system = RAGFactory.create_rag_system(engine_name)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ {engine_name}: {e}")
            return {"error": str(e)}
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ —Ç–µ—Å—Ç—ã
        performance_result = self.run_performance_benchmark(rag_system, engine_name)
        quality_result = self.run_quality_benchmark(rag_system, engine_name)
        load_test_result = self.run_load_test(rag_system)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        full_result = {
            "engine": engine_name,
            "timestamp": datetime.now().isoformat(),
            "performance": performance_result,
            "quality": quality_result,
            "load_test": load_test_result
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.save_results(full_result, engine_name)
        
        return full_result

    def save_results(self, results: Dict[str, Any], engine_name: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã benchmark –≤ —Ñ–∞–π–ª—ã"""
        # JSON —Ñ–∞–π–ª
        json_file = f"{self.output_dir}/benchmark_{engine_name}_{self.timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        # CSV —Ñ–∞–π–ª —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if "performance" in results and "performance_results" in results["performance"]:
            perf_df = pd.DataFrame(results["performance"]["performance_results"])
            csv_file = f"{self.output_dir}/performance_{engine_name}_{self.timestamp}.csv"
            perf_df.to_csv(csv_file, index=False, encoding='utf-8')
        
        # CSV —Ñ–∞–π–ª —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        if "quality" in results and "quality_results" in results["quality"]:
            qual_df = pd.DataFrame(results["quality"]["quality_results"])
            csv_file = f"{self.output_dir}/quality_{engine_name}_{self.timestamp}.csv"
            qual_df.to_csv(csv_file, index=False, encoding='utf-8')
        
        print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {self.output_dir}/")

    def compare_engines(self, engines: List[str] = ["baseline"]) -> Dict[str, Any]:
        """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —Ä–∞–∑–Ω—ã–µ –¥–≤–∏–∂–∫–∏ RAG"""
        print("üîÑ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–≤–∏–∂–∫–æ–≤ RAG")
        print("=" * 60)
        
        comparison_results = {}
        
        for engine in engines:
            print(f"\nüîß –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {engine}...")
            try:
                result = self.run_full_benchmark(engine)
                comparison_results[engine] = result
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ {engine}: {e}")
                comparison_results[engine] = {"error": str(e)}
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
        comparison_summary = self.create_comparison_summary(comparison_results)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
        comparison_file = f"{self.output_dir}/comparison_{self.timestamp}.json"
        with open(comparison_file, 'w', encoding='utf-8') as f:
            json.dump(comparison_results, f, ensure_ascii=False, indent=2)
        
        return comparison_results

    def create_comparison_summary(self, comparison_results: Dict[str, Any]) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–µ—Ç —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –¥–≤–∏–∂–∫–æ–≤"""
        summary_data = []
        
        for engine, results in comparison_results.items():
            if "error" in results:
                continue
                
            perf = results.get("performance", {})
            qual = results.get("quality", {})
            load = results.get("load_test", {})
            
            summary_data.append({
                "Engine": engine,
                "Avg Query Time (s)": perf.get("avg_query_time", 0),
                "Success Rate": perf.get("avg_success_rate", 0),
                "Keyword Score": qual.get("avg_keyword_score", 0),
                "Source Score": qual.get("avg_source_score", 0),
                "Queries/Second": load.get("queries_per_second", 0),
                "Load Success Rate": load.get("success_rate", 0)
            })
        
        summary_df = pd.DataFrame(summary_data)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
        summary_file = f"{self.output_dir}/comparison_summary_{self.timestamp}.csv"
        summary_df.to_csv(summary_file, index=False, encoding='utf-8')
        
        print("\nüìä –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è:")
        print(summary_df.to_string(index=False))
        
        return summary_df

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ benchmark"""
    print("üéØ RAG System Benchmark")
    print("=" * 60)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    required_vars = ['OPENAI_API_KEY', 'PINECONE_API_KEY', 'PINECONE_INDEX_NAME']
    missing_vars = [var for var in required_vars if not os.getenv(var)]
    
    if missing_vars:
        print(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è: {', '.join(missing_vars)}")
        return
    
    # –°–æ–∑–¥–∞–µ–º benchmark
    benchmark = RAGBenchmark()
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º benchmark –¥–ª—è baseline
    print("üöÄ –ó–∞–ø—É—Å–∫ benchmark –¥–ª—è baseline –¥–≤–∏–∂–∫–∞...")
    baseline_result = benchmark.run_full_benchmark("baseline")
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å –¥—Ä—É–≥–∏–µ –¥–≤–∏–∂–∫–∏, —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∏—Ö
    available_engines = ["baseline"]  # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å "graphrag", "lightrag"
    
    if len(available_engines) > 1:
        print("\nüîÑ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–≤–∏–∂–∫–æ–≤...")
        comparison_results = benchmark.compare_engines(available_engines)
    
    print("\nüéâ Benchmark –∑–∞–≤–µ—Ä—à–µ–Ω!")
    print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {benchmark.output_dir}/")

if __name__ == "__main__":
    main()
