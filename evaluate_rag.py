import time
import json
from typing import List, Dict, Any
from rag_system import rag_system

# Sample test queries for legal domain
TEST_QUERIES = [
    "ĞšĞ°ĞºĞ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ° Ğ¸Ğ¼ĞµĞµÑ‚ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ½Ğ¸Ğº Ğ¿Ñ€Ğ¸ ÑƒĞ²Ğ¾Ğ»ÑŒĞ½ĞµĞ½Ğ¸Ğ¸?",
    "ĞšĞ°Ğº Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ‚ÑŒ Ğ´Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ñ€ ĞºÑƒĞ¿Ğ»Ğ¸-Ğ¿Ñ€Ğ¾Ğ´Ğ°Ğ¶Ğ¸ Ğ½ĞµĞ´Ğ²Ğ¸Ğ¶Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸?",
    "ĞšĞ°ĞºĞ¸Ğµ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ½ÑƒĞ¶Ğ½Ñ‹ Ğ´Ğ»Ñ Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸ Ğ˜ĞŸ?",
    "ĞšĞ°Ğº Ğ¿Ğ¾Ğ´Ğ°Ñ‚ÑŒ Ğ¸ÑĞº Ğ² ÑÑƒĞ´?",
    "ĞšĞ°ĞºĞ¸Ğµ Ğ½Ğ°Ğ»Ğ¾Ğ³Ğ¸ Ğ¿Ğ»Ğ°Ñ‚Ğ¸Ñ‚ Ñ„Ğ¸Ğ·Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ Ğ»Ğ¸Ñ†Ğ¾?",
    "ĞšĞ°Ğº Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ¸Ñ‚ÑŒ Ğ°Ğ²Ñ‚Ğ¾Ñ€ÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ°?",
    "ĞšĞ°ĞºĞ¸Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ñ‚Ñ€ÑƒĞ´Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ° Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹?",
    "ĞšĞ°Ğº Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ‚ÑŒ Ğ½Ğ°ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¾?",
    "ĞšĞ°ĞºĞ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ° Ñƒ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ¸Ñ‚ĞµĞ»Ñ Ğ¿Ñ€Ğ¸ Ğ¿Ğ¾ĞºÑƒĞ¿ĞºĞµ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ°?",
    "ĞšĞ°Ğº Ğ¿Ğ¾Ğ´Ğ°Ñ‚ÑŒ Ğ¶Ğ°Ğ»Ğ¾Ğ±Ñƒ Ğ½Ğ° Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ Ñ‡Ğ¸Ğ½Ğ¾Ğ²Ğ½Ğ¸ĞºĞ¾Ğ²?"
]

def evaluate_query(query: str, use_hybrid: bool = True, use_rerank: bool = True) -> Dict[str, Any]:
    """Evaluate a single query"""
    start_time = time.time()
    
    try:
        result = rag_system.query(
            user_query=query,
            use_hybrid_search=use_hybrid,
            use_reranking=use_rerank
        )
        
        end_time = time.time()
        response_time = end_time - start_time
        
        return {
            "query": query,
            "success": True,
            "response_time": response_time,
            "answer_length": len(result["answer"]),
            "sources_count": len(result["sources"]),
            "results_count": result["results_count"],
            "context_length": result["context_length"],
            "has_answer": len(result["answer"]) > 50,
            "has_sources": len(result["sources"]) > 0
        }
        
    except Exception as e:
        end_time = time.time()
        return {
            "query": query,
            "success": False,
            "response_time": end_time - start_time,
            "error": str(e)
        }

def run_evaluation():
    """Run comprehensive evaluation"""
    print("ğŸ§ª EVALUATION OF ENHANCED RAG SYSTEM")
    print("=" * 60)
    
    # Test different configurations
    configurations = [
        ("Simple Search", False, False),
        ("Dense Search", False, True),
        ("Hybrid Search", True, True)
    ]
    
    results = {}
    
    for config_name, use_hybrid, use_rerank in configurations:
        print(f"\nğŸ” Testing: {config_name}")
        print("-" * 40)
        
        config_results = []
        total_time = 0
        success_count = 0
        
        for i, query in enumerate(TEST_QUERIES, 1):
            print(f"  {i}/{len(TEST_QUERIES)}: {query[:50]}...")
            
            result = evaluate_query(query, use_hybrid, use_rerank)
            config_results.append(result)
            
            if result["success"]:
                success_count += 1
                total_time += result["response_time"]
        
        # Calculate metrics
        avg_time = total_time / success_count if success_count > 0 else 0
        success_rate = success_count / len(TEST_QUERIES)
        
        # Quality metrics
        avg_answer_length = sum(r["answer_length"] for r in config_results if r["success"]) / success_count if success_count > 0 else 0
        avg_sources = sum(r["sources_count"] for r in config_results if r["success"]) / success_count if success_count > 0 else 0
        
        results[config_name] = {
            "success_rate": success_rate,
            "avg_response_time": avg_time,
            "avg_answer_length": avg_answer_length,
            "avg_sources": avg_sources,
            "detailed_results": config_results
        }
        
        print(f"  âœ… Success Rate: {success_rate:.2%}")
        print(f"  â±ï¸  Avg Response Time: {avg_time:.2f}s")
        print(f"  ğŸ“ Avg Answer Length: {avg_answer_length:.0f} chars")
        print(f"  ğŸ“š Avg Sources: {avg_sources:.1f}")
    
    # Print comparison
    print("\nğŸ“Š COMPARISON SUMMARY")
    print("=" * 60)
    print(f"{'Configuration':<20} {'Success':<10} {'Time(s)':<10} {'Length':<10} {'Sources':<10}")
    print("-" * 60)
    
    for config_name, metrics in results.items():
        print(f"{config_name:<20} {metrics['success_rate']:<10.1%} {metrics['avg_response_time']:<10.2f} "
              f"{metrics['avg_answer_length']:<10.0f} {metrics['avg_sources']:<10.1f}")
    
    # Save detailed results
    with open("evaluation_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“„ Detailed results saved to: evaluation_results.json")
    
    return results

def test_conversation_memory():
    """Test conversation memory functionality"""
    print("\nğŸ’¬ TESTING CONVERSATION MEMORY")
    print("=" * 40)
    
    # Clear history first
    rag_system.clear_conversation_history()
    
    # Simulate conversation
    conversation = [
        "Ğ§Ñ‚Ğ¾ Ñ‚Ğ°ĞºĞ¾Ğµ Ñ‚Ñ€ÑƒĞ´Ğ¾Ğ²Ğ¾Ğ¹ Ğ´Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ñ€?",
        "ĞšĞ°ĞºĞ¸Ğµ Ğ¾Ğ±ÑĞ·Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ¾Ğ½ Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ÑŒ?",
        "ĞœĞ¾Ğ¶Ğ½Ğ¾ Ğ»Ğ¸ Ñ€Ğ°ÑÑ‚Ğ¾Ñ€Ğ³Ğ½ÑƒÑ‚ÑŒ Ğ´Ğ¾Ğ³Ğ¾Ğ²Ğ¾Ñ€ Ğ´Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ğ¾?"
    ]
    
    for i, query in enumerate(conversation, 1):
        print(f"\n{i}. Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ: {query}")
        result = rag_system.query(query)
        print(f"   ĞÑ‚Ğ²ĞµÑ‚: {result['answer'][:100]}...")
        print(f"   Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸: {len(result['sources'])}")
    
    # Check history
    history = rag_system.get_conversation_history()
    print(f"\nğŸ“œ Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ {len(history)} Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹")
    
    return history

def test_system_stats():
    """Test system statistics"""
    print("\nğŸ“Š SYSTEM STATISTICS")
    print("=" * 30)
    
    stats = rag_system.get_system_stats()
    for key, value in stats.items():
        if isinstance(value, dict):
            print(f"\n{key}:")
            for sub_key, sub_value in value.items():
                print(f"  {sub_key}: {sub_value}")
        else:
            print(f"{key}: {value}")

if __name__ == "__main__":
    # Run evaluation
    evaluation_results = run_evaluation()
    
    # Test conversation memory
    conversation_history = test_conversation_memory()
    
    # Test system stats
    test_system_stats()
    
    print("\nâœ… Evaluation completed!") 