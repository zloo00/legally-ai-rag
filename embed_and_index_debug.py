import os
import json
import openai
from dotenv import load_dotenv
from pinecone import Pinecone, ServerlessSpec
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
import numpy as np

# === –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ –∫–ª—é—á–µ–π ===
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_ENVIRONMENT = os.getenv("PINECONE_ENVIRONMENT") or "us-east-1"
INDEX_NAME = os.getenv("PINECONE_INDEX_NAME") or "legally-index"

print(f"üîß Configuration:")
print(f"   INDEX_NAME: {INDEX_NAME}")
print(f"   PINECONE_ENVIRONMENT: {PINECONE_ENVIRONMENT}")

# === –®–∞–≥ 2: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤ ===
openai.api_key = OPENAI_API_KEY
pc = Pinecone(api_key=PINECONE_API_KEY)

# Initialize sentence transformer for local embeddings
sentence_model = SentenceTransformer('all-MiniLM-L6-v2')

# === –®–∞–≥ 3: –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞, –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç ===
try:
    existing_indexes = [index.name for index in pc.list_indexes()]
    print(f"üìã Existing indexes: {existing_indexes}")
    
    if INDEX_NAME not in existing_indexes:
        print(f"üìé –ò–Ω–¥–µ–∫—Å '{INDEX_NAME}' –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π...")
        pc.create_index(
            name=INDEX_NAME,
            dimension=1536,  # OpenAI embedding dimension
            metric="cosine",
            spec=ServerlessSpec(cloud="aws", region=PINECONE_ENVIRONMENT)
        )
        print(f"‚úÖ –ò–Ω–¥–µ–∫—Å '{INDEX_NAME}' —Å–æ–∑–¥–∞–Ω.")
    else:
        print(f"‚úÖ –ò–Ω–¥–µ–∫—Å '{INDEX_NAME}' —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")
        
    index = pc.Index(INDEX_NAME)
    print(f"‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∏–Ω–¥–µ–∫—Å—É —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ.")
    
except Exception as e:
    print(f"‚ùå Error with Pinecone index: {e}")
    exit(1)

# === –®–∞–≥ 4: –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ ===
chunk_dir = "data/chunks"
texts = []
metadatas = []

print(f"\nüìñ –ó–∞–≥—Ä—É–∑–∫–∞ —á–∞–Ω–∫–æ–≤ –∏–∑ {chunk_dir}...")

chunk_files = [f for f in os.listdir(chunk_dir) if f.endswith(".txt") and not f.endswith("_meta.txt")]
print(f"üìÅ Found {len(chunk_files)} chunk files")

for filename in chunk_files:
    path = os.path.join(chunk_dir, filename)
    meta_path = os.path.join(chunk_dir, filename.replace(".txt", "_meta.txt"))
    
    try:
        with open(path, "r", encoding="utf-8") as f:
            text = f.read().strip()
        
        if len(text) < 10:
            print(f"‚ö†Ô∏è  Skipping {filename}: too short ({len(text)} chars)")
            continue
            
        # Load metadata if available
        metadata = {"filename": filename, "text": text[:200] + "..." if len(text) > 200 else text}
        
        if os.path.exists(meta_path):
            with open(meta_path, "r", encoding="utf-8") as mf:
                for line in mf:
                    if ":" in line:
                        key, value = line.strip().split(":", 1)
                        metadata[key] = value
        
        texts.append(text)
        metadatas.append(metadata)
        
    except Exception as e:
        print(f"‚ùå Error loading {filename}: {e}")
        continue

print(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(texts)} —á–∞–Ω–∫–æ–≤")

if len(texts) == 0:
    print("‚ùå No texts loaded! Exiting.")
    exit(1)

# === –®–∞–≥ 5: –§—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ ===
def get_embedding(text, model="text-embedding-3-small"):
    """Get embedding from OpenAI"""
    try:
        text = text.replace("\n", " ")
        client = openai.OpenAI(api_key=OPENAI_API_KEY)
        res = client.embeddings.create(input=[text], model=model)
        return res.data[0].embedding
    except Exception as e:
        print(f"‚ùå Error getting OpenAI embedding: {e}")
        return None

def get_local_embedding(text):
    """Get embedding using sentence transformers"""
    try:
        embedding = sentence_model.encode(text)
        return embedding.tolist()
    except Exception as e:
        print(f"‚ùå Error getting local embedding: {e}")
        return None

# === –®–∞–≥ 6: –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –≤ Pinecone ===
batch_size = 10  # Smaller batch size for debugging

print(f"\nüöÄ –ù–∞—á–∏–Ω–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é —Å batch_size={batch_size}...")

successful_uploads = 0
failed_uploads = 0
detailed_errors = []

for i in tqdm(range(0, len(texts), batch_size), desc="üì¶ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ Pinecone"):
    batch_texts = texts[i:i + batch_size]
    batch_metadatas = metadatas[i:i + batch_size]
    
    vectors_to_upsert = []
    
    for j, (text, metadata) in enumerate(zip(batch_texts, batch_metadatas)):
        try:
            # Get OpenAI embedding
            embedding = get_embedding(text)
            
            if embedding is None:
                error_msg = f"Failed to get embedding for {metadata.get('filename', 'unknown')}"
                detailed_errors.append(error_msg)
                failed_uploads += 1
                continue
                
            # Get local embedding for hybrid search
            local_embedding = get_local_embedding(text)
            
            # Enhanced metadata
            enhanced_metadata = {
                **metadata,
                "text_length": len(text),
                "embedding_model": "text-embedding-3-small"
            }
            
            # Remove local_embedding from metadata if it's too large
            if local_embedding and len(local_embedding) > 1000:
                enhanced_metadata["has_local_embedding"] = True
            else:
                enhanced_metadata["local_embedding"] = local_embedding if local_embedding else []
            
            vectors_to_upsert.append({
                "id": f"doc-{i + j}",
                "values": embedding,
                "metadata": enhanced_metadata
            })
            
        except Exception as e:
            error_msg = f"Error processing {metadata.get('filename', 'unknown')}: {e}"
            detailed_errors.append(error_msg)
            failed_uploads += 1
            continue
    
    # Upload batch
    if vectors_to_upsert:
        try:
            print(f"\nüì§ Uploading batch {i//batch_size + 1} with {len(vectors_to_upsert)} vectors...")
            index.upsert(vectors=vectors_to_upsert)
            successful_uploads += len(vectors_to_upsert)
            print(f"‚úÖ Batch {i//batch_size + 1} uploaded successfully!")
        except Exception as e:
            error_msg = f"Error uploading batch {i//batch_size + 1}: {e}"
            detailed_errors.append(error_msg)
            print(f"‚ùå {error_msg}")
            failed_uploads += len(vectors_to_upsert)

print(f"\n‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
print(f"üìä –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {successful_uploads}")
print(f"‚ùå –û—à–∏–±–æ–∫: {failed_uploads}")

# Save detailed error log
if detailed_errors:
    with open("embedding_errors.log", "w", encoding="utf-8") as f:
        f.write("Embedding Errors Log\n")
        f.write("=" * 50 + "\n")
        for error in detailed_errors:
            f.write(f"{error}\n")
    print(f"üìù Detailed errors saved to embedding_errors.log")

# Save index statistics
stats = {
    "total_chunks": len(texts),
    "successful_uploads": successful_uploads,
    "failed_uploads": failed_uploads,
    "index_name": INDEX_NAME,
    "errors": detailed_errors[:10]  # Save first 10 errors
}

with open("index_stats.json", "w") as f:
    json.dump(stats, f, indent=2)

print("üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ index_stats.json") 